# AI Agent 量化评估维度 (Quantification Framework)

在面试中，当你提到“我的 Agent 准确率有 95%”时，面试官一定会追问：“你是怎么算出来的？”。以下是工业界的标准量化方法：

## 1. 核心指标 (The RAG Triad)

| 指标名称 | 英文 | 量化定义 | 解决的问题 |
| :--- | :--- | :--- | :--- |
| **工具准确率** | Tool-Call Accuracy | `正确工具调用数 / 总测试用例数` | Agent 逻辑是否走对 |
| **忠实度** | Faithfulness | `回答中来源于检索上下文的陈述数 / 总陈述数` | 幻觉 (Hallucination) |
| **回答相关性** | Answer Relevance | `回答与原始问题的语义相似度 (0-1)` | 答非所问 |
| **检索召回率** | Context Recall | `检索到的关键信息点 / 数据库中存在的关键点` | 漏掉关键信息 |

## 2. 如何利用 LLM 自动评分 (LLM-as-a-Judge)

量化不是拍脑袋，而是通过“裁判模型”给出的分数。

### 示例逻辑：
假设我们要量化 **Faithfulness (忠实度)**，我们会给裁判 LLM 发送以下指令：
> **Prompt**: 
> - 给定背景: "香港布会大学图书馆 GSR 1 房间有 6 个座位。"
> - Agent 回答: "好的，我已经为你预定了 GSR 1，它很大，有 10 个座位。"
> - **量化任务**: 请拆解 Agent 回答中的事实，并检查每个事实是否在背景中：
>   1. 动作是预定 GSR 1 (Yes)
>   2. 座位是 10 个 (No)
> - **计算评分**: 1 个正确 / 2 个事实 = **0.5分**

## 3. 你的 95% 是怎么来的？(面试话术模板)

你可以这样量化你的项目：

> “我构建了一个包含 **50 个测试用例** 的验证集。
> 1. **回归测试 (Regression)**: 每次我修改 Prompt 后，脚本会自动运行这 50 个用例。
> 2. **分项评分**: 
>    - 如果 `tool_name` 匹配，得 1.0 分；
>    - 如果 `parameters` (时间、人数) 提取正确，得 1.0 分；
>    - 否则得 0 分。
> 3. **最终汇总**: 在最近的一次测试中，有 48 个用例完全正确，2 个用例在‘提取跨天时间’时失败，因此我的 **Tool Accuracy = 48/50 = 96%**。”

## 4. 进阶：端到端延迟 (Latency)

除了准确性，量化还包括**性能**：
- **TTFT (Time to First Token)**: 用户输入到模型吐出第一个字的时间（衡量响应速度）。
- **Cost (成本)**: 平均每次订位消耗的 Token 数。

---

**建议**：你可以试着在 `evaluate_agent.py` 里手动把 `average_score` 的计算公式改得更复杂一点，比如把“提到了房间号”给更高的权重，这就是你自己的“量化逻辑”了。
